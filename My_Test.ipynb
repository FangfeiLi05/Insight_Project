{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "1erjV-kg1ioy"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eh3CXk8O1lYP",
    "outputId": "03a2b42d-90c2-40a6-8ffa-11450afb92f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Set hyperparameters\n",
    "\n",
    "num_classes = 10     # number of output classes discrete range [0,9]\n",
    "num_epochs = 100     # number of times the entire dataset is presented to the model\n",
    "batch_size = 64      # size of input data took for one iteration\n",
    "lr = 1e-3            # learning rate\n",
    "weight_decay = 1e-4  # regularization parameter\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rPBQAAiH1pnG",
    "outputId": "f90d527c-8620-4c99-b033-06547b3ce441"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:06, 26539724.85it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download CIFAR-10 and set up dataloaders\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "train_data = dsets.CIFAR10(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transform, download=True)\n",
    "\n",
    "test_data = dsets.CIFAR10(root='./data', train=False,\n",
    "                          transform=transform, download=True)\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "test_gen = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "JQS1nhnM2Ejx"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.4)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc = nn.Linear(128*4*4, num_classes)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conv layer\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc   \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "w6thuFLo-Ckz",
    "outputId": "d71b279a-18b7-4c90-c93a-896cf575f089"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout2d(p=0.2)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace)\n",
       "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace)\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout2d(p=0.3)\n",
       "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout2d(p=0.4)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "net = Net(num_classes)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "5IGRaDLw2F5_"
   },
   "outputs": [],
   "source": [
    "# Define loss function & optimizer\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, \n",
    "                            weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g54EGFkA2Qan",
    "outputId": "c65eaf68-0b02-4c9c-dc96-af54e70cc812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/781], Loss: 1.7074\n",
      "Epoch [1/100], Step [200/781], Loss: 1.5957\n",
      "Epoch [1/100], Step [300/781], Loss: 1.5788\n",
      "Epoch [1/100], Step [400/781], Loss: 1.3265\n",
      "Epoch [1/100], Step [500/781], Loss: 1.3113\n",
      "Epoch [1/100], Step [600/781], Loss: 1.2482\n",
      "Epoch [1/100], Step [700/781], Loss: 1.2045\n",
      "Epoch [2/100], Step [100/781], Loss: 1.0107\n",
      "Epoch [2/100], Step [200/781], Loss: 0.9745\n",
      "Epoch [2/100], Step [300/781], Loss: 0.9252\n",
      "Epoch [2/100], Step [400/781], Loss: 0.8418\n",
      "Epoch [2/100], Step [500/781], Loss: 0.9159\n",
      "Epoch [2/100], Step [600/781], Loss: 1.0478\n",
      "Epoch [2/100], Step [700/781], Loss: 0.8980\n",
      "Epoch [3/100], Step [100/781], Loss: 0.8531\n",
      "Epoch [3/100], Step [200/781], Loss: 0.7905\n",
      "Epoch [3/100], Step [300/781], Loss: 0.9442\n",
      "Epoch [3/100], Step [400/781], Loss: 0.8169\n",
      "Epoch [3/100], Step [500/781], Loss: 0.8185\n",
      "Epoch [3/100], Step [600/781], Loss: 0.8521\n",
      "Epoch [3/100], Step [700/781], Loss: 0.6792\n",
      "Epoch [4/100], Step [100/781], Loss: 0.8388\n",
      "Epoch [4/100], Step [200/781], Loss: 0.9400\n",
      "Epoch [4/100], Step [300/781], Loss: 0.7406\n",
      "Epoch [4/100], Step [400/781], Loss: 0.7837\n",
      "Epoch [4/100], Step [500/781], Loss: 0.9436\n",
      "Epoch [4/100], Step [600/781], Loss: 0.9763\n",
      "Epoch [4/100], Step [700/781], Loss: 0.8488\n",
      "Epoch [5/100], Step [100/781], Loss: 0.8799\n",
      "Epoch [5/100], Step [200/781], Loss: 0.5423\n",
      "Epoch [5/100], Step [300/781], Loss: 0.6859\n",
      "Epoch [5/100], Step [400/781], Loss: 0.7388\n",
      "Epoch [5/100], Step [500/781], Loss: 0.7873\n",
      "Epoch [5/100], Step [600/781], Loss: 0.6911\n",
      "Epoch [5/100], Step [700/781], Loss: 0.8181\n",
      "Epoch [6/100], Step [100/781], Loss: 0.8542\n",
      "Epoch [6/100], Step [200/781], Loss: 0.7580\n",
      "Epoch [6/100], Step [300/781], Loss: 0.8290\n",
      "Epoch [6/100], Step [400/781], Loss: 0.6609\n",
      "Epoch [6/100], Step [500/781], Loss: 1.0823\n",
      "Epoch [6/100], Step [600/781], Loss: 0.8486\n",
      "Epoch [6/100], Step [700/781], Loss: 0.8286\n",
      "Epoch [7/100], Step [100/781], Loss: 0.6038\n",
      "Epoch [7/100], Step [200/781], Loss: 0.7234\n",
      "Epoch [7/100], Step [300/781], Loss: 0.6230\n",
      "Epoch [7/100], Step [400/781], Loss: 0.6678\n",
      "Epoch [7/100], Step [500/781], Loss: 0.5909\n",
      "Epoch [7/100], Step [600/781], Loss: 0.3726\n",
      "Epoch [7/100], Step [700/781], Loss: 0.4820\n",
      "Epoch [8/100], Step [100/781], Loss: 0.7007\n",
      "Epoch [8/100], Step [200/781], Loss: 0.5455\n",
      "Epoch [8/100], Step [300/781], Loss: 0.5165\n",
      "Epoch [8/100], Step [400/781], Loss: 0.4915\n",
      "Epoch [8/100], Step [500/781], Loss: 0.3948\n",
      "Epoch [8/100], Step [600/781], Loss: 0.7796\n",
      "Epoch [8/100], Step [700/781], Loss: 0.5107\n",
      "Epoch [9/100], Step [100/781], Loss: 0.4896\n",
      "Epoch [9/100], Step [200/781], Loss: 0.9824\n",
      "Epoch [9/100], Step [300/781], Loss: 0.9388\n",
      "Epoch [9/100], Step [400/781], Loss: 0.6078\n",
      "Epoch [9/100], Step [500/781], Loss: 0.7290\n",
      "Epoch [9/100], Step [600/781], Loss: 0.7928\n",
      "Epoch [9/100], Step [700/781], Loss: 0.5991\n",
      "Epoch [10/100], Step [100/781], Loss: 0.6090\n",
      "Epoch [10/100], Step [200/781], Loss: 0.4234\n",
      "Epoch [10/100], Step [300/781], Loss: 0.4413\n",
      "Epoch [10/100], Step [400/781], Loss: 0.5986\n",
      "Epoch [10/100], Step [500/781], Loss: 0.2777\n",
      "Epoch [10/100], Step [600/781], Loss: 0.4630\n",
      "Epoch [10/100], Step [700/781], Loss: 0.4268\n",
      "Epoch [11/100], Step [100/781], Loss: 0.3978\n",
      "Epoch [11/100], Step [200/781], Loss: 0.5729\n",
      "Epoch [11/100], Step [300/781], Loss: 0.8115\n",
      "Epoch [11/100], Step [400/781], Loss: 0.4373\n",
      "Epoch [11/100], Step [500/781], Loss: 0.5901\n",
      "Epoch [11/100], Step [600/781], Loss: 0.6131\n",
      "Epoch [11/100], Step [700/781], Loss: 0.6047\n",
      "Epoch [12/100], Step [100/781], Loss: 0.3252\n",
      "Epoch [12/100], Step [200/781], Loss: 0.5967\n",
      "Epoch [12/100], Step [300/781], Loss: 0.3880\n",
      "Epoch [12/100], Step [400/781], Loss: 0.3946\n",
      "Epoch [12/100], Step [500/781], Loss: 0.4744\n",
      "Epoch [12/100], Step [600/781], Loss: 0.3498\n",
      "Epoch [12/100], Step [700/781], Loss: 0.5495\n",
      "Epoch [13/100], Step [100/781], Loss: 0.4362\n",
      "Epoch [13/100], Step [200/781], Loss: 0.5307\n",
      "Epoch [13/100], Step [300/781], Loss: 0.5277\n",
      "Epoch [13/100], Step [400/781], Loss: 0.5905\n",
      "Epoch [13/100], Step [500/781], Loss: 0.5051\n",
      "Epoch [13/100], Step [600/781], Loss: 0.6059\n",
      "Epoch [13/100], Step [700/781], Loss: 0.3613\n",
      "Epoch [14/100], Step [100/781], Loss: 0.5845\n",
      "Epoch [14/100], Step [200/781], Loss: 0.5361\n",
      "Epoch [14/100], Step [300/781], Loss: 0.5255\n",
      "Epoch [14/100], Step [400/781], Loss: 0.4702\n",
      "Epoch [14/100], Step [500/781], Loss: 0.3681\n",
      "Epoch [14/100], Step [600/781], Loss: 0.4673\n",
      "Epoch [14/100], Step [700/781], Loss: 0.5652\n",
      "Epoch [15/100], Step [100/781], Loss: 0.6697\n",
      "Epoch [15/100], Step [200/781], Loss: 0.4992\n",
      "Epoch [15/100], Step [300/781], Loss: 0.5317\n",
      "Epoch [15/100], Step [400/781], Loss: 0.2446\n",
      "Epoch [15/100], Step [500/781], Loss: 0.4052\n",
      "Epoch [15/100], Step [600/781], Loss: 0.5226\n",
      "Epoch [15/100], Step [700/781], Loss: 0.5852\n",
      "Epoch [16/100], Step [100/781], Loss: 0.6739\n",
      "Epoch [16/100], Step [200/781], Loss: 0.5093\n",
      "Epoch [16/100], Step [300/781], Loss: 0.4020\n",
      "Epoch [16/100], Step [400/781], Loss: 0.4712\n",
      "Epoch [16/100], Step [500/781], Loss: 0.6342\n",
      "Epoch [16/100], Step [600/781], Loss: 0.6087\n",
      "Epoch [16/100], Step [700/781], Loss: 0.5680\n",
      "Epoch [17/100], Step [100/781], Loss: 0.3531\n",
      "Epoch [17/100], Step [200/781], Loss: 0.3246\n",
      "Epoch [17/100], Step [300/781], Loss: 0.3140\n",
      "Epoch [17/100], Step [400/781], Loss: 0.4588\n",
      "Epoch [17/100], Step [500/781], Loss: 0.4284\n",
      "Epoch [17/100], Step [600/781], Loss: 0.3953\n",
      "Epoch [17/100], Step [700/781], Loss: 0.5898\n",
      "Epoch [18/100], Step [100/781], Loss: 0.3522\n",
      "Epoch [18/100], Step [200/781], Loss: 0.2865\n",
      "Epoch [18/100], Step [300/781], Loss: 0.5881\n",
      "Epoch [18/100], Step [400/781], Loss: 0.3212\n",
      "Epoch [18/100], Step [500/781], Loss: 0.5539\n",
      "Epoch [18/100], Step [600/781], Loss: 0.3675\n",
      "Epoch [18/100], Step [700/781], Loss: 0.3015\n",
      "Epoch [19/100], Step [100/781], Loss: 0.3667\n",
      "Epoch [19/100], Step [200/781], Loss: 0.6209\n",
      "Epoch [19/100], Step [300/781], Loss: 0.4671\n",
      "Epoch [19/100], Step [400/781], Loss: 0.1579\n",
      "Epoch [19/100], Step [500/781], Loss: 0.5144\n",
      "Epoch [19/100], Step [600/781], Loss: 0.3460\n",
      "Epoch [19/100], Step [700/781], Loss: 0.4073\n",
      "Epoch [20/100], Step [100/781], Loss: 0.3905\n",
      "Epoch [20/100], Step [200/781], Loss: 0.5389\n",
      "Epoch [20/100], Step [300/781], Loss: 0.4128\n",
      "Epoch [20/100], Step [400/781], Loss: 0.3529\n",
      "Epoch [20/100], Step [500/781], Loss: 0.4331\n",
      "Epoch [20/100], Step [600/781], Loss: 0.4350\n",
      "Epoch [20/100], Step [700/781], Loss: 0.6138\n",
      "Epoch [21/100], Step [100/781], Loss: 0.5545\n",
      "Epoch [21/100], Step [200/781], Loss: 0.2849\n",
      "Epoch [21/100], Step [300/781], Loss: 0.3134\n",
      "Epoch [21/100], Step [400/781], Loss: 0.5180\n",
      "Epoch [21/100], Step [500/781], Loss: 0.3852\n",
      "Epoch [21/100], Step [600/781], Loss: 0.2517\n",
      "Epoch [21/100], Step [700/781], Loss: 0.3843\n",
      "Epoch [22/100], Step [100/781], Loss: 0.3803\n",
      "Epoch [22/100], Step [200/781], Loss: 0.3329\n",
      "Epoch [22/100], Step [300/781], Loss: 0.3521\n",
      "Epoch [22/100], Step [400/781], Loss: 0.5499\n",
      "Epoch [22/100], Step [500/781], Loss: 0.1769\n",
      "Epoch [22/100], Step [600/781], Loss: 0.4584\n",
      "Epoch [22/100], Step [700/781], Loss: 0.4066\n",
      "Epoch [23/100], Step [100/781], Loss: 0.2553\n",
      "Epoch [23/100], Step [200/781], Loss: 0.5268\n",
      "Epoch [23/100], Step [300/781], Loss: 0.4157\n",
      "Epoch [23/100], Step [400/781], Loss: 0.3726\n",
      "Epoch [23/100], Step [500/781], Loss: 0.5149\n",
      "Epoch [23/100], Step [600/781], Loss: 0.2550\n",
      "Epoch [23/100], Step [700/781], Loss: 0.2681\n",
      "Epoch [24/100], Step [100/781], Loss: 0.4604\n",
      "Epoch [24/100], Step [200/781], Loss: 0.3180\n",
      "Epoch [24/100], Step [300/781], Loss: 0.3233\n",
      "Epoch [24/100], Step [400/781], Loss: 0.4606\n",
      "Epoch [24/100], Step [500/781], Loss: 0.2836\n",
      "Epoch [24/100], Step [600/781], Loss: 0.3422\n",
      "Epoch [24/100], Step [700/781], Loss: 0.3819\n",
      "Epoch [25/100], Step [100/781], Loss: 0.3618\n",
      "Epoch [25/100], Step [200/781], Loss: 0.2816\n",
      "Epoch [25/100], Step [300/781], Loss: 0.5392\n",
      "Epoch [25/100], Step [400/781], Loss: 0.5237\n",
      "Epoch [25/100], Step [500/781], Loss: 0.3558\n",
      "Epoch [25/100], Step [600/781], Loss: 0.2731\n",
      "Epoch [25/100], Step [700/781], Loss: 0.2732\n",
      "Epoch [26/100], Step [100/781], Loss: 0.3467\n",
      "Epoch [26/100], Step [200/781], Loss: 0.3915\n",
      "Epoch [26/100], Step [300/781], Loss: 0.3265\n",
      "Epoch [26/100], Step [400/781], Loss: 0.4611\n",
      "Epoch [26/100], Step [500/781], Loss: 0.3888\n",
      "Epoch [26/100], Step [600/781], Loss: 0.4803\n",
      "Epoch [26/100], Step [700/781], Loss: 0.2730\n",
      "Epoch [27/100], Step [100/781], Loss: 0.4522\n",
      "Epoch [27/100], Step [200/781], Loss: 0.3134\n",
      "Epoch [27/100], Step [300/781], Loss: 0.5052\n",
      "Epoch [27/100], Step [400/781], Loss: 0.3607\n",
      "Epoch [27/100], Step [500/781], Loss: 0.1542\n",
      "Epoch [27/100], Step [600/781], Loss: 0.4472\n",
      "Epoch [27/100], Step [700/781], Loss: 0.5398\n",
      "Epoch [28/100], Step [100/781], Loss: 0.2984\n",
      "Epoch [28/100], Step [200/781], Loss: 0.4845\n",
      "Epoch [28/100], Step [300/781], Loss: 0.4149\n",
      "Epoch [28/100], Step [400/781], Loss: 0.3362\n",
      "Epoch [28/100], Step [500/781], Loss: 0.4583\n",
      "Epoch [28/100], Step [600/781], Loss: 0.2697\n",
      "Epoch [28/100], Step [700/781], Loss: 0.4138\n",
      "Epoch [29/100], Step [100/781], Loss: 0.2734\n",
      "Epoch [29/100], Step [200/781], Loss: 0.4376\n",
      "Epoch [29/100], Step [300/781], Loss: 0.4107\n",
      "Epoch [29/100], Step [400/781], Loss: 0.2353\n",
      "Epoch [29/100], Step [500/781], Loss: 0.3312\n",
      "Epoch [29/100], Step [600/781], Loss: 0.3846\n",
      "Epoch [29/100], Step [700/781], Loss: 0.2858\n",
      "Epoch [30/100], Step [100/781], Loss: 0.3857\n",
      "Epoch [30/100], Step [200/781], Loss: 0.6461\n",
      "Epoch [30/100], Step [300/781], Loss: 0.2236\n",
      "Epoch [30/100], Step [400/781], Loss: 0.4359\n",
      "Epoch [30/100], Step [500/781], Loss: 0.2845\n",
      "Epoch [30/100], Step [600/781], Loss: 0.4192\n",
      "Epoch [30/100], Step [700/781], Loss: 0.4915\n",
      "Epoch [31/100], Step [100/781], Loss: 0.2155\n",
      "Epoch [31/100], Step [200/781], Loss: 0.2406\n",
      "Epoch [31/100], Step [300/781], Loss: 0.2001\n",
      "Epoch [31/100], Step [400/781], Loss: 0.2371\n",
      "Epoch [31/100], Step [500/781], Loss: 0.3154\n",
      "Epoch [31/100], Step [600/781], Loss: 0.2656\n",
      "Epoch [31/100], Step [700/781], Loss: 0.3219\n",
      "Epoch [32/100], Step [100/781], Loss: 0.4680\n",
      "Epoch [32/100], Step [200/781], Loss: 0.2070\n",
      "Epoch [32/100], Step [300/781], Loss: 0.3733\n",
      "Epoch [32/100], Step [400/781], Loss: 0.2923\n",
      "Epoch [32/100], Step [500/781], Loss: 0.2601\n",
      "Epoch [32/100], Step [600/781], Loss: 0.3098\n",
      "Epoch [32/100], Step [700/781], Loss: 0.3181\n",
      "Epoch [33/100], Step [100/781], Loss: 0.3651\n",
      "Epoch [33/100], Step [200/781], Loss: 0.2277\n",
      "Epoch [33/100], Step [300/781], Loss: 0.4221\n",
      "Epoch [33/100], Step [400/781], Loss: 0.5441\n",
      "Epoch [33/100], Step [500/781], Loss: 0.2808\n",
      "Epoch [33/100], Step [600/781], Loss: 0.4789\n",
      "Epoch [33/100], Step [700/781], Loss: 0.4059\n",
      "Epoch [34/100], Step [100/781], Loss: 0.4849\n",
      "Epoch [34/100], Step [200/781], Loss: 0.2710\n",
      "Epoch [34/100], Step [300/781], Loss: 0.3735\n",
      "Epoch [34/100], Step [400/781], Loss: 0.2498\n",
      "Epoch [34/100], Step [500/781], Loss: 0.4233\n",
      "Epoch [34/100], Step [600/781], Loss: 0.2949\n",
      "Epoch [34/100], Step [700/781], Loss: 0.2059\n",
      "Epoch [35/100], Step [100/781], Loss: 0.2958\n",
      "Epoch [35/100], Step [200/781], Loss: 0.2181\n",
      "Epoch [35/100], Step [300/781], Loss: 0.2943\n",
      "Epoch [35/100], Step [400/781], Loss: 0.2195\n",
      "Epoch [35/100], Step [500/781], Loss: 0.2956\n",
      "Epoch [35/100], Step [600/781], Loss: 0.1068\n",
      "Epoch [35/100], Step [700/781], Loss: 0.2043\n",
      "Epoch [36/100], Step [100/781], Loss: 0.2716\n",
      "Epoch [36/100], Step [200/781], Loss: 0.2033\n",
      "Epoch [36/100], Step [300/781], Loss: 0.4034\n",
      "Epoch [36/100], Step [400/781], Loss: 0.3722\n",
      "Epoch [36/100], Step [500/781], Loss: 0.3362\n",
      "Epoch [36/100], Step [600/781], Loss: 0.3319\n",
      "Epoch [36/100], Step [700/781], Loss: 0.2260\n",
      "Epoch [37/100], Step [100/781], Loss: 0.3166\n",
      "Epoch [37/100], Step [200/781], Loss: 0.1616\n",
      "Epoch [37/100], Step [300/781], Loss: 0.4994\n",
      "Epoch [37/100], Step [400/781], Loss: 0.3164\n",
      "Epoch [37/100], Step [500/781], Loss: 0.3685\n",
      "Epoch [37/100], Step [600/781], Loss: 0.3555\n",
      "Epoch [37/100], Step [700/781], Loss: 0.2246\n",
      "Epoch [38/100], Step [100/781], Loss: 0.2075\n",
      "Epoch [38/100], Step [200/781], Loss: 0.1648\n",
      "Epoch [38/100], Step [300/781], Loss: 0.5598\n",
      "Epoch [38/100], Step [400/781], Loss: 0.3233\n",
      "Epoch [38/100], Step [500/781], Loss: 0.3681\n",
      "Epoch [38/100], Step [600/781], Loss: 0.2431\n",
      "Epoch [38/100], Step [700/781], Loss: 0.3415\n",
      "Epoch [39/100], Step [100/781], Loss: 0.1877\n",
      "Epoch [39/100], Step [200/781], Loss: 0.2438\n",
      "Epoch [39/100], Step [300/781], Loss: 0.3647\n",
      "Epoch [39/100], Step [400/781], Loss: 0.2100\n",
      "Epoch [39/100], Step [500/781], Loss: 0.3006\n",
      "Epoch [39/100], Step [600/781], Loss: 0.5325\n",
      "Epoch [39/100], Step [700/781], Loss: 0.2942\n",
      "Epoch [40/100], Step [100/781], Loss: 0.2795\n",
      "Epoch [40/100], Step [200/781], Loss: 0.1510\n",
      "Epoch [40/100], Step [300/781], Loss: 0.4274\n",
      "Epoch [40/100], Step [400/781], Loss: 0.4026\n",
      "Epoch [40/100], Step [500/781], Loss: 0.2036\n",
      "Epoch [40/100], Step [600/781], Loss: 0.2564\n",
      "Epoch [40/100], Step [700/781], Loss: 0.5794\n",
      "Epoch [41/100], Step [100/781], Loss: 0.3012\n",
      "Epoch [41/100], Step [200/781], Loss: 0.2069\n",
      "Epoch [41/100], Step [300/781], Loss: 0.2860\n",
      "Epoch [41/100], Step [400/781], Loss: 0.2464\n",
      "Epoch [41/100], Step [500/781], Loss: 0.2734\n",
      "Epoch [41/100], Step [600/781], Loss: 0.2660\n",
      "Epoch [41/100], Step [700/781], Loss: 0.3990\n",
      "Epoch [42/100], Step [100/781], Loss: 0.2743\n",
      "Epoch [42/100], Step [200/781], Loss: 0.1911\n",
      "Epoch [42/100], Step [300/781], Loss: 0.2932\n",
      "Epoch [42/100], Step [400/781], Loss: 0.4451\n",
      "Epoch [42/100], Step [500/781], Loss: 0.2809\n",
      "Epoch [42/100], Step [600/781], Loss: 0.4303\n",
      "Epoch [42/100], Step [700/781], Loss: 0.2067\n",
      "Epoch [43/100], Step [100/781], Loss: 0.4368\n",
      "Epoch [43/100], Step [200/781], Loss: 0.2247\n",
      "Epoch [43/100], Step [300/781], Loss: 0.2687\n",
      "Epoch [43/100], Step [400/781], Loss: 0.2870\n",
      "Epoch [43/100], Step [500/781], Loss: 0.3033\n",
      "Epoch [43/100], Step [600/781], Loss: 0.3085\n",
      "Epoch [43/100], Step [700/781], Loss: 0.2137\n",
      "Epoch [44/100], Step [100/781], Loss: 0.3318\n",
      "Epoch [44/100], Step [200/781], Loss: 0.2418\n",
      "Epoch [44/100], Step [300/781], Loss: 0.4237\n",
      "Epoch [44/100], Step [400/781], Loss: 0.4121\n",
      "Epoch [44/100], Step [500/781], Loss: 0.1933\n",
      "Epoch [44/100], Step [600/781], Loss: 0.4144\n",
      "Epoch [44/100], Step [700/781], Loss: 0.3254\n",
      "Epoch [45/100], Step [100/781], Loss: 0.2434\n",
      "Epoch [45/100], Step [200/781], Loss: 0.2131\n",
      "Epoch [45/100], Step [300/781], Loss: 0.1753\n",
      "Epoch [45/100], Step [400/781], Loss: 0.3201\n",
      "Epoch [45/100], Step [500/781], Loss: 0.3799\n",
      "Epoch [45/100], Step [600/781], Loss: 0.1996\n",
      "Epoch [45/100], Step [700/781], Loss: 0.6279\n",
      "Epoch [46/100], Step [100/781], Loss: 0.2489\n",
      "Epoch [46/100], Step [200/781], Loss: 0.3805\n",
      "Epoch [46/100], Step [300/781], Loss: 0.4251\n",
      "Epoch [46/100], Step [400/781], Loss: 0.2227\n",
      "Epoch [46/100], Step [500/781], Loss: 0.3570\n",
      "Epoch [46/100], Step [600/781], Loss: 0.2292\n",
      "Epoch [46/100], Step [700/781], Loss: 0.1679\n",
      "Epoch [47/100], Step [100/781], Loss: 0.1286\n",
      "Epoch [47/100], Step [200/781], Loss: 0.1765\n",
      "Epoch [47/100], Step [300/781], Loss: 0.2918\n",
      "Epoch [47/100], Step [400/781], Loss: 0.3376\n",
      "Epoch [47/100], Step [500/781], Loss: 0.3838\n",
      "Epoch [47/100], Step [600/781], Loss: 0.2349\n",
      "Epoch [47/100], Step [700/781], Loss: 0.2802\n",
      "Epoch [48/100], Step [100/781], Loss: 0.1793\n",
      "Epoch [48/100], Step [200/781], Loss: 0.1772\n",
      "Epoch [48/100], Step [300/781], Loss: 0.3851\n",
      "Epoch [48/100], Step [400/781], Loss: 0.1856\n",
      "Epoch [48/100], Step [500/781], Loss: 0.1813\n",
      "Epoch [48/100], Step [600/781], Loss: 0.2952\n",
      "Epoch [48/100], Step [700/781], Loss: 0.4280\n",
      "Epoch [49/100], Step [100/781], Loss: 0.1753\n",
      "Epoch [49/100], Step [200/781], Loss: 0.4034\n",
      "Epoch [49/100], Step [300/781], Loss: 0.3354\n",
      "Epoch [49/100], Step [400/781], Loss: 0.2366\n",
      "Epoch [49/100], Step [500/781], Loss: 0.2156\n",
      "Epoch [49/100], Step [600/781], Loss: 0.2559\n",
      "Epoch [49/100], Step [700/781], Loss: 0.3010\n",
      "Epoch [50/100], Step [100/781], Loss: 0.1544\n",
      "Epoch [50/100], Step [200/781], Loss: 0.1589\n",
      "Epoch [50/100], Step [300/781], Loss: 0.3600\n",
      "Epoch [50/100], Step [400/781], Loss: 0.3540\n",
      "Epoch [50/100], Step [500/781], Loss: 0.2020\n",
      "Epoch [50/100], Step [600/781], Loss: 0.2913\n",
      "Epoch [50/100], Step [700/781], Loss: 0.2413\n",
      "Epoch [51/100], Step [100/781], Loss: 0.3547\n",
      "Epoch [51/100], Step [200/781], Loss: 0.2703\n",
      "Epoch [51/100], Step [300/781], Loss: 0.2725\n",
      "Epoch [51/100], Step [400/781], Loss: 0.1173\n",
      "Epoch [51/100], Step [500/781], Loss: 0.2443\n",
      "Epoch [51/100], Step [600/781], Loss: 0.2997\n",
      "Epoch [51/100], Step [700/781], Loss: 0.2830\n",
      "Epoch [52/100], Step [100/781], Loss: 0.1506\n",
      "Epoch [52/100], Step [200/781], Loss: 0.5734\n",
      "Epoch [52/100], Step [300/781], Loss: 0.2131\n",
      "Epoch [52/100], Step [400/781], Loss: 0.4166\n",
      "Epoch [52/100], Step [500/781], Loss: 0.1062\n",
      "Epoch [52/100], Step [600/781], Loss: 0.2348\n",
      "Epoch [52/100], Step [700/781], Loss: 0.2340\n",
      "Epoch [53/100], Step [100/781], Loss: 0.3182\n",
      "Epoch [53/100], Step [200/781], Loss: 0.5395\n",
      "Epoch [53/100], Step [300/781], Loss: 0.2381\n",
      "Epoch [53/100], Step [400/781], Loss: 0.3895\n",
      "Epoch [53/100], Step [500/781], Loss: 0.3656\n",
      "Epoch [53/100], Step [600/781], Loss: 0.2110\n",
      "Epoch [53/100], Step [700/781], Loss: 0.3782\n",
      "Epoch [54/100], Step [100/781], Loss: 0.2157\n",
      "Epoch [54/100], Step [200/781], Loss: 0.3407\n",
      "Epoch [54/100], Step [300/781], Loss: 0.3312\n",
      "Epoch [54/100], Step [400/781], Loss: 0.3464\n",
      "Epoch [54/100], Step [500/781], Loss: 0.3611\n",
      "Epoch [54/100], Step [600/781], Loss: 0.3258\n",
      "Epoch [54/100], Step [700/781], Loss: 0.2725\n",
      "Epoch [55/100], Step [100/781], Loss: 0.1475\n",
      "Epoch [55/100], Step [200/781], Loss: 0.1085\n",
      "Epoch [55/100], Step [300/781], Loss: 0.1939\n",
      "Epoch [55/100], Step [400/781], Loss: 0.5631\n",
      "Epoch [55/100], Step [500/781], Loss: 0.1611\n",
      "Epoch [55/100], Step [600/781], Loss: 0.1996\n",
      "Epoch [55/100], Step [700/781], Loss: 0.1876\n",
      "Epoch [56/100], Step [100/781], Loss: 0.2108\n",
      "Epoch [56/100], Step [200/781], Loss: 0.2529\n",
      "Epoch [56/100], Step [300/781], Loss: 0.1904\n",
      "Epoch [56/100], Step [400/781], Loss: 0.3437\n",
      "Epoch [56/100], Step [500/781], Loss: 0.1599\n",
      "Epoch [56/100], Step [600/781], Loss: 0.4008\n",
      "Epoch [56/100], Step [700/781], Loss: 0.4739\n",
      "Epoch [57/100], Step [100/781], Loss: 0.0828\n",
      "Epoch [57/100], Step [200/781], Loss: 0.2264\n",
      "Epoch [57/100], Step [300/781], Loss: 0.3675\n",
      "Epoch [57/100], Step [400/781], Loss: 0.1378\n",
      "Epoch [57/100], Step [500/781], Loss: 0.1979\n",
      "Epoch [57/100], Step [600/781], Loss: 0.3171\n",
      "Epoch [57/100], Step [700/781], Loss: 0.3178\n",
      "Epoch [58/100], Step [100/781], Loss: 0.3913\n",
      "Epoch [58/100], Step [200/781], Loss: 0.3129\n",
      "Epoch [58/100], Step [300/781], Loss: 0.1665\n",
      "Epoch [58/100], Step [400/781], Loss: 0.2058\n",
      "Epoch [58/100], Step [500/781], Loss: 0.4759\n",
      "Epoch [58/100], Step [600/781], Loss: 0.3110\n",
      "Epoch [58/100], Step [700/781], Loss: 0.3421\n",
      "Epoch [59/100], Step [100/781], Loss: 0.2615\n",
      "Epoch [59/100], Step [200/781], Loss: 0.1883\n",
      "Epoch [59/100], Step [300/781], Loss: 0.3196\n",
      "Epoch [59/100], Step [400/781], Loss: 0.1919\n",
      "Epoch [59/100], Step [500/781], Loss: 0.1527\n",
      "Epoch [59/100], Step [600/781], Loss: 0.2709\n",
      "Epoch [59/100], Step [700/781], Loss: 0.1593\n",
      "Epoch [60/100], Step [100/781], Loss: 0.1526\n",
      "Epoch [60/100], Step [200/781], Loss: 0.2304\n",
      "Epoch [60/100], Step [300/781], Loss: 0.3467\n",
      "Epoch [60/100], Step [400/781], Loss: 0.2921\n",
      "Epoch [60/100], Step [500/781], Loss: 0.2361\n",
      "Epoch [60/100], Step [600/781], Loss: 0.2861\n",
      "Epoch [60/100], Step [700/781], Loss: 0.2715\n",
      "Epoch [61/100], Step [100/781], Loss: 0.1490\n",
      "Epoch [61/100], Step [200/781], Loss: 0.2648\n",
      "Epoch [61/100], Step [300/781], Loss: 0.2657\n",
      "Epoch [61/100], Step [400/781], Loss: 0.2554\n",
      "Epoch [61/100], Step [500/781], Loss: 0.1451\n",
      "Epoch [61/100], Step [600/781], Loss: 0.2098\n",
      "Epoch [61/100], Step [700/781], Loss: 0.1715\n",
      "Epoch [62/100], Step [100/781], Loss: 0.1624\n",
      "Epoch [62/100], Step [200/781], Loss: 0.2173\n",
      "Epoch [62/100], Step [300/781], Loss: 0.2934\n",
      "Epoch [62/100], Step [400/781], Loss: 0.2471\n",
      "Epoch [62/100], Step [500/781], Loss: 0.2223\n",
      "Epoch [62/100], Step [600/781], Loss: 0.3831\n",
      "Epoch [62/100], Step [700/781], Loss: 0.3671\n",
      "Epoch [63/100], Step [100/781], Loss: 0.2191\n",
      "Epoch [63/100], Step [200/781], Loss: 0.2733\n",
      "Epoch [63/100], Step [300/781], Loss: 0.1990\n",
      "Epoch [63/100], Step [400/781], Loss: 0.2224\n",
      "Epoch [63/100], Step [500/781], Loss: 0.1567\n",
      "Epoch [63/100], Step [600/781], Loss: 0.3199\n",
      "Epoch [63/100], Step [700/781], Loss: 0.2836\n",
      "Epoch [64/100], Step [100/781], Loss: 0.2979\n",
      "Epoch [64/100], Step [200/781], Loss: 0.2187\n",
      "Epoch [64/100], Step [300/781], Loss: 0.2778\n",
      "Epoch [64/100], Step [400/781], Loss: 0.1934\n",
      "Epoch [64/100], Step [500/781], Loss: 0.2530\n",
      "Epoch [64/100], Step [600/781], Loss: 0.1369\n",
      "Epoch [64/100], Step [700/781], Loss: 0.4420\n",
      "Epoch [65/100], Step [100/781], Loss: 0.2097\n",
      "Epoch [65/100], Step [200/781], Loss: 0.3151\n",
      "Epoch [65/100], Step [300/781], Loss: 0.2400\n",
      "Epoch [65/100], Step [400/781], Loss: 0.2724\n",
      "Epoch [65/100], Step [500/781], Loss: 0.1564\n",
      "Epoch [65/100], Step [600/781], Loss: 0.1936\n",
      "Epoch [65/100], Step [700/781], Loss: 0.2695\n",
      "Epoch [66/100], Step [100/781], Loss: 0.2250\n",
      "Epoch [66/100], Step [200/781], Loss: 0.2490\n",
      "Epoch [66/100], Step [300/781], Loss: 0.5023\n",
      "Epoch [66/100], Step [400/781], Loss: 0.3659\n",
      "Epoch [66/100], Step [500/781], Loss: 0.3379\n",
      "Epoch [66/100], Step [600/781], Loss: 0.2587\n",
      "Epoch [66/100], Step [700/781], Loss: 0.2590\n",
      "Epoch [67/100], Step [100/781], Loss: 0.1938\n",
      "Epoch [67/100], Step [200/781], Loss: 0.2109\n",
      "Epoch [67/100], Step [300/781], Loss: 0.2799\n",
      "Epoch [67/100], Step [400/781], Loss: 0.4037\n",
      "Epoch [67/100], Step [500/781], Loss: 0.1838\n",
      "Epoch [67/100], Step [600/781], Loss: 0.2531\n",
      "Epoch [67/100], Step [700/781], Loss: 0.1201\n",
      "Epoch [68/100], Step [100/781], Loss: 0.2956\n",
      "Epoch [68/100], Step [200/781], Loss: 0.3765\n",
      "Epoch [68/100], Step [300/781], Loss: 0.2694\n",
      "Epoch [68/100], Step [400/781], Loss: 0.1962\n",
      "Epoch [68/100], Step [500/781], Loss: 0.3483\n",
      "Epoch [68/100], Step [600/781], Loss: 0.2593\n",
      "Epoch [68/100], Step [700/781], Loss: 0.1556\n",
      "Epoch [69/100], Step [100/781], Loss: 0.1814\n",
      "Epoch [69/100], Step [200/781], Loss: 0.4861\n",
      "Epoch [69/100], Step [300/781], Loss: 0.1758\n",
      "Epoch [69/100], Step [400/781], Loss: 0.1802\n",
      "Epoch [69/100], Step [500/781], Loss: 0.2274\n",
      "Epoch [69/100], Step [600/781], Loss: 0.2315\n",
      "Epoch [69/100], Step [700/781], Loss: 0.4862\n",
      "Epoch [70/100], Step [100/781], Loss: 0.0961\n",
      "Epoch [70/100], Step [200/781], Loss: 0.1369\n",
      "Epoch [70/100], Step [300/781], Loss: 0.1517\n",
      "Epoch [70/100], Step [400/781], Loss: 0.2886\n",
      "Epoch [70/100], Step [500/781], Loss: 0.3688\n",
      "Epoch [70/100], Step [600/781], Loss: 0.3923\n",
      "Epoch [70/100], Step [700/781], Loss: 0.1451\n",
      "Epoch [71/100], Step [100/781], Loss: 0.3286\n",
      "Epoch [71/100], Step [200/781], Loss: 0.1403\n",
      "Epoch [71/100], Step [300/781], Loss: 0.2346\n",
      "Epoch [71/100], Step [400/781], Loss: 0.2314\n",
      "Epoch [71/100], Step [500/781], Loss: 0.1473\n",
      "Epoch [71/100], Step [600/781], Loss: 0.2247\n",
      "Epoch [71/100], Step [700/781], Loss: 0.1926\n",
      "Epoch [72/100], Step [100/781], Loss: 0.3168\n",
      "Epoch [72/100], Step [200/781], Loss: 0.1554\n",
      "Epoch [72/100], Step [300/781], Loss: 0.3827\n",
      "Epoch [72/100], Step [400/781], Loss: 0.2197\n",
      "Epoch [72/100], Step [500/781], Loss: 0.2302\n",
      "Epoch [72/100], Step [600/781], Loss: 0.1704\n",
      "Epoch [72/100], Step [700/781], Loss: 0.2171\n",
      "Epoch [73/100], Step [100/781], Loss: 0.2402\n",
      "Epoch [73/100], Step [200/781], Loss: 0.3442\n",
      "Epoch [73/100], Step [300/781], Loss: 0.2733\n",
      "Epoch [73/100], Step [400/781], Loss: 0.1397\n",
      "Epoch [73/100], Step [500/781], Loss: 0.1808\n",
      "Epoch [73/100], Step [600/781], Loss: 0.1599\n",
      "Epoch [73/100], Step [700/781], Loss: 0.1525\n",
      "Epoch [74/100], Step [100/781], Loss: 0.2288\n",
      "Epoch [74/100], Step [200/781], Loss: 0.1692\n",
      "Epoch [74/100], Step [300/781], Loss: 0.2675\n",
      "Epoch [74/100], Step [400/781], Loss: 0.1916\n",
      "Epoch [74/100], Step [500/781], Loss: 0.3345\n",
      "Epoch [74/100], Step [600/781], Loss: 0.1902\n",
      "Epoch [74/100], Step [700/781], Loss: 0.1769\n",
      "Epoch [75/100], Step [100/781], Loss: 0.2420\n",
      "Epoch [75/100], Step [200/781], Loss: 0.0753\n",
      "Epoch [75/100], Step [300/781], Loss: 0.1696\n",
      "Epoch [75/100], Step [400/781], Loss: 0.1401\n",
      "Epoch [75/100], Step [500/781], Loss: 0.0864\n",
      "Epoch [75/100], Step [600/781], Loss: 0.3940\n",
      "Epoch [75/100], Step [700/781], Loss: 0.2023\n",
      "Epoch [76/100], Step [100/781], Loss: 0.3585\n",
      "Epoch [76/100], Step [200/781], Loss: 0.3449\n",
      "Epoch [76/100], Step [300/781], Loss: 0.2624\n",
      "Epoch [76/100], Step [400/781], Loss: 0.1864\n",
      "Epoch [76/100], Step [500/781], Loss: 0.3352\n",
      "Epoch [76/100], Step [600/781], Loss: 0.2106\n",
      "Epoch [76/100], Step [700/781], Loss: 0.3244\n",
      "Epoch [77/100], Step [100/781], Loss: 0.3239\n",
      "Epoch [77/100], Step [200/781], Loss: 0.1739\n",
      "Epoch [77/100], Step [300/781], Loss: 0.1773\n",
      "Epoch [77/100], Step [400/781], Loss: 0.2902\n",
      "Epoch [77/100], Step [500/781], Loss: 0.4899\n",
      "Epoch [77/100], Step [600/781], Loss: 0.2463\n",
      "Epoch [77/100], Step [700/781], Loss: 0.3264\n",
      "Epoch [78/100], Step [100/781], Loss: 0.1708\n",
      "Epoch [78/100], Step [200/781], Loss: 0.2322\n",
      "Epoch [78/100], Step [300/781], Loss: 0.1920\n",
      "Epoch [78/100], Step [400/781], Loss: 0.2487\n",
      "Epoch [78/100], Step [500/781], Loss: 0.1688\n",
      "Epoch [78/100], Step [600/781], Loss: 0.2790\n",
      "Epoch [78/100], Step [700/781], Loss: 0.1613\n",
      "Epoch [79/100], Step [100/781], Loss: 0.3103\n",
      "Epoch [79/100], Step [200/781], Loss: 0.2433\n",
      "Epoch [79/100], Step [300/781], Loss: 0.1853\n",
      "Epoch [79/100], Step [400/781], Loss: 0.2499\n",
      "Epoch [79/100], Step [500/781], Loss: 0.2042\n",
      "Epoch [79/100], Step [600/781], Loss: 0.2314\n",
      "Epoch [79/100], Step [700/781], Loss: 0.5013\n",
      "Epoch [80/100], Step [100/781], Loss: 0.2020\n",
      "Epoch [80/100], Step [200/781], Loss: 0.3737\n",
      "Epoch [80/100], Step [300/781], Loss: 0.3592\n",
      "Epoch [80/100], Step [400/781], Loss: 0.2710\n",
      "Epoch [80/100], Step [500/781], Loss: 0.1322\n",
      "Epoch [80/100], Step [600/781], Loss: 0.3770\n",
      "Epoch [80/100], Step [700/781], Loss: 0.4510\n",
      "Epoch [81/100], Step [100/781], Loss: 0.2772\n",
      "Epoch [81/100], Step [200/781], Loss: 0.1682\n",
      "Epoch [81/100], Step [300/781], Loss: 0.3466\n",
      "Epoch [81/100], Step [400/781], Loss: 0.2914\n",
      "Epoch [81/100], Step [500/781], Loss: 0.1296\n",
      "Epoch [81/100], Step [600/781], Loss: 0.2583\n",
      "Epoch [81/100], Step [700/781], Loss: 0.3199\n",
      "Epoch [82/100], Step [100/781], Loss: 0.1757\n",
      "Epoch [82/100], Step [200/781], Loss: 0.6204\n",
      "Epoch [82/100], Step [300/781], Loss: 0.3004\n",
      "Epoch [82/100], Step [400/781], Loss: 0.2585\n",
      "Epoch [82/100], Step [500/781], Loss: 0.3164\n",
      "Epoch [82/100], Step [600/781], Loss: 0.3512\n",
      "Epoch [82/100], Step [700/781], Loss: 0.1576\n",
      "Epoch [83/100], Step [100/781], Loss: 0.2652\n",
      "Epoch [83/100], Step [200/781], Loss: 0.2218\n",
      "Epoch [83/100], Step [300/781], Loss: 0.1699\n",
      "Epoch [83/100], Step [400/781], Loss: 0.2356\n",
      "Epoch [83/100], Step [500/781], Loss: 0.3112\n",
      "Epoch [83/100], Step [600/781], Loss: 0.1524\n",
      "Epoch [83/100], Step [700/781], Loss: 0.1585\n",
      "Epoch [84/100], Step [100/781], Loss: 0.1300\n",
      "Epoch [84/100], Step [200/781], Loss: 0.1009\n",
      "Epoch [84/100], Step [300/781], Loss: 0.2908\n",
      "Epoch [84/100], Step [400/781], Loss: 0.3099\n",
      "Epoch [84/100], Step [500/781], Loss: 0.1559\n",
      "Epoch [84/100], Step [600/781], Loss: 0.2637\n",
      "Epoch [84/100], Step [700/781], Loss: 0.2710\n",
      "Epoch [85/100], Step [100/781], Loss: 0.1283\n",
      "Epoch [85/100], Step [200/781], Loss: 0.1181\n",
      "Epoch [85/100], Step [300/781], Loss: 0.1951\n",
      "Epoch [85/100], Step [400/781], Loss: 0.2056\n",
      "Epoch [85/100], Step [500/781], Loss: 0.4142\n",
      "Epoch [85/100], Step [600/781], Loss: 0.2906\n",
      "Epoch [85/100], Step [700/781], Loss: 0.2343\n",
      "Epoch [86/100], Step [100/781], Loss: 0.2519\n",
      "Epoch [86/100], Step [200/781], Loss: 0.1441\n",
      "Epoch [86/100], Step [300/781], Loss: 0.1913\n",
      "Epoch [86/100], Step [400/781], Loss: 0.2392\n",
      "Epoch [86/100], Step [500/781], Loss: 0.2166\n",
      "Epoch [86/100], Step [600/781], Loss: 0.4680\n",
      "Epoch [86/100], Step [700/781], Loss: 0.3198\n",
      "Epoch [87/100], Step [100/781], Loss: 0.2278\n",
      "Epoch [87/100], Step [200/781], Loss: 0.1663\n",
      "Epoch [87/100], Step [300/781], Loss: 0.2557\n",
      "Epoch [87/100], Step [400/781], Loss: 0.1949\n",
      "Epoch [87/100], Step [500/781], Loss: 0.2350\n",
      "Epoch [87/100], Step [600/781], Loss: 0.3238\n",
      "Epoch [87/100], Step [700/781], Loss: 0.1845\n",
      "Epoch [88/100], Step [100/781], Loss: 0.2659\n",
      "Epoch [88/100], Step [200/781], Loss: 0.2099\n",
      "Epoch [88/100], Step [300/781], Loss: 0.1819\n",
      "Epoch [88/100], Step [400/781], Loss: 0.2240\n",
      "Epoch [88/100], Step [500/781], Loss: 0.2960\n",
      "Epoch [88/100], Step [600/781], Loss: 0.1081\n",
      "Epoch [88/100], Step [700/781], Loss: 0.2958\n",
      "Epoch [89/100], Step [100/781], Loss: 0.1605\n",
      "Epoch [89/100], Step [200/781], Loss: 0.2878\n",
      "Epoch [89/100], Step [300/781], Loss: 0.1445\n",
      "Epoch [89/100], Step [400/781], Loss: 0.1341\n",
      "Epoch [89/100], Step [500/781], Loss: 0.1299\n",
      "Epoch [89/100], Step [600/781], Loss: 0.3178\n",
      "Epoch [89/100], Step [700/781], Loss: 0.3760\n",
      "Epoch [90/100], Step [100/781], Loss: 0.1664\n",
      "Epoch [90/100], Step [200/781], Loss: 0.1274\n",
      "Epoch [90/100], Step [300/781], Loss: 0.3011\n",
      "Epoch [90/100], Step [400/781], Loss: 0.3288\n",
      "Epoch [90/100], Step [500/781], Loss: 0.2134\n",
      "Epoch [90/100], Step [600/781], Loss: 0.3392\n",
      "Epoch [90/100], Step [700/781], Loss: 0.3056\n",
      "Epoch [91/100], Step [100/781], Loss: 0.3060\n",
      "Epoch [91/100], Step [200/781], Loss: 0.4166\n",
      "Epoch [91/100], Step [300/781], Loss: 0.2142\n",
      "Epoch [91/100], Step [400/781], Loss: 0.1818\n",
      "Epoch [91/100], Step [500/781], Loss: 0.1407\n",
      "Epoch [91/100], Step [600/781], Loss: 0.2114\n",
      "Epoch [91/100], Step [700/781], Loss: 0.3074\n",
      "Epoch [92/100], Step [100/781], Loss: 0.2651\n",
      "Epoch [92/100], Step [200/781], Loss: 0.2534\n",
      "Epoch [92/100], Step [300/781], Loss: 0.2959\n",
      "Epoch [92/100], Step [400/781], Loss: 0.2557\n",
      "Epoch [92/100], Step [500/781], Loss: 0.2357\n",
      "Epoch [92/100], Step [600/781], Loss: 0.2734\n",
      "Epoch [92/100], Step [700/781], Loss: 0.2367\n",
      "Epoch [93/100], Step [100/781], Loss: 0.2089\n",
      "Epoch [93/100], Step [200/781], Loss: 0.3470\n",
      "Epoch [93/100], Step [300/781], Loss: 0.3648\n",
      "Epoch [93/100], Step [400/781], Loss: 0.1394\n",
      "Epoch [93/100], Step [500/781], Loss: 0.2479\n",
      "Epoch [93/100], Step [600/781], Loss: 0.3529\n",
      "Epoch [93/100], Step [700/781], Loss: 0.1697\n",
      "Epoch [94/100], Step [100/781], Loss: 0.2276\n",
      "Epoch [94/100], Step [200/781], Loss: 0.2691\n",
      "Epoch [94/100], Step [300/781], Loss: 0.2112\n",
      "Epoch [94/100], Step [400/781], Loss: 0.2146\n",
      "Epoch [94/100], Step [500/781], Loss: 0.3265\n",
      "Epoch [94/100], Step [600/781], Loss: 0.1919\n",
      "Epoch [94/100], Step [700/781], Loss: 0.2451\n",
      "Epoch [95/100], Step [100/781], Loss: 0.1872\n",
      "Epoch [95/100], Step [200/781], Loss: 0.1096\n",
      "Epoch [95/100], Step [300/781], Loss: 0.1089\n",
      "Epoch [95/100], Step [400/781], Loss: 0.2985\n",
      "Epoch [95/100], Step [500/781], Loss: 0.0604\n",
      "Epoch [95/100], Step [600/781], Loss: 0.1675\n",
      "Epoch [95/100], Step [700/781], Loss: 0.2141\n",
      "Epoch [96/100], Step [100/781], Loss: 0.2745\n",
      "Epoch [96/100], Step [200/781], Loss: 0.1296\n",
      "Epoch [96/100], Step [300/781], Loss: 0.2143\n",
      "Epoch [96/100], Step [400/781], Loss: 0.3537\n",
      "Epoch [96/100], Step [500/781], Loss: 0.2246\n",
      "Epoch [96/100], Step [600/781], Loss: 0.1669\n",
      "Epoch [96/100], Step [700/781], Loss: 0.2602\n",
      "Epoch [97/100], Step [100/781], Loss: 0.2968\n",
      "Epoch [97/100], Step [200/781], Loss: 0.1246\n",
      "Epoch [97/100], Step [300/781], Loss: 0.1804\n",
      "Epoch [97/100], Step [400/781], Loss: 0.2517\n",
      "Epoch [97/100], Step [500/781], Loss: 0.1835\n",
      "Epoch [97/100], Step [600/781], Loss: 0.1539\n",
      "Epoch [97/100], Step [700/781], Loss: 0.1782\n",
      "Epoch [98/100], Step [100/781], Loss: 0.1962\n",
      "Epoch [98/100], Step [200/781], Loss: 0.1081\n",
      "Epoch [98/100], Step [300/781], Loss: 0.0420\n",
      "Epoch [98/100], Step [400/781], Loss: 0.2442\n",
      "Epoch [98/100], Step [500/781], Loss: 0.2781\n",
      "Epoch [98/100], Step [600/781], Loss: 0.1837\n",
      "Epoch [98/100], Step [700/781], Loss: 0.1550\n",
      "Epoch [99/100], Step [100/781], Loss: 0.2800\n",
      "Epoch [99/100], Step [200/781], Loss: 0.3168\n",
      "Epoch [99/100], Step [300/781], Loss: 0.2599\n",
      "Epoch [99/100], Step [400/781], Loss: 0.2582\n",
      "Epoch [99/100], Step [500/781], Loss: 0.1896\n",
      "Epoch [99/100], Step [600/781], Loss: 0.2324\n",
      "Epoch [99/100], Step [700/781], Loss: 0.1123\n",
      "Epoch [100/100], Step [100/781], Loss: 0.2266\n",
      "Epoch [100/100], Step [200/781], Loss: 0.2076\n",
      "Epoch [100/100], Step [300/781], Loss: 0.1329\n",
      "Epoch [100/100], Step [400/781], Loss: 0.4542\n",
      "Epoch [100/100], Step [500/781], Loss: 0.3004\n",
      "Epoch [100/100], Step [600/781], Loss: 0.2792\n",
      "Epoch [100/100], Step [700/781], Loss: 0.3292\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_gen):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                  %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, \n",
    "                    loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "_ASrSfGPJviZ",
    "outputId": "87f9f21d-d40c-4383-ffd3-27867335477e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.498409\n",
      "\n",
      "Test Accuracy of airplane: 85% (199/234)\n",
      "Test Accuracy of automobile: 93% (229/246)\n",
      "Test Accuracy of  bird: 80% (227/281)\n",
      "Test Accuracy of   cat: 66% (169/253)\n",
      "Test Accuracy of  deer: 87% (199/227)\n",
      "Test Accuracy of   dog: 80% (201/250)\n",
      "Test Accuracy of  frog: 87% (218/248)\n",
      "Test Accuracy of horse: 90% (227/250)\n",
      "Test Accuracy of  ship: 93% (242/258)\n",
      "Test Accuracy of truck: 93% (248/265)\n",
      "\n",
      "Test Accuracy (Overall): 85% (2159/2512)\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on test set\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "net.eval()\n",
    "\n",
    "for images, labels in test_gen:\n",
    "    if torch.cuda.is_available():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "    outputs = net(images)\n",
    "  \n",
    "    loss = loss_function(outputs, labels)\n",
    "    test_loss += loss.item()*images.size(0)\n",
    "  \n",
    "    _, pred = torch.max(outputs, 1)    \n",
    "  \n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "  \n",
    "    for i in range(16):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "test_loss = test_loss/len(test_gen.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KO3RgIDq2I-4"
   },
   "outputs": [],
   "source": [
    "# Run model on test set and save predictions\n",
    "\n",
    "net.eval()\n",
    "predictions = []\n",
    "\n",
    "for images, _ in test_gen:\n",
    "    images = images.to(device)\n",
    "\n",
    "    output = net(images)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    img_id_start = len(predictions) + 1\n",
    "    predictions += [{'image_id': img_id_start + x, \n",
    "                     'label': predicted[x].item()} for x in range(len(predicted))]\n",
    "\n",
    "with open('caliper_cifar10_test_predictions_FangfeiLi.csv', mode='w') as preds_file:\n",
    "    writer = csv.writer(preds_file, delimiter=',')\n",
    "    writer.writerow(['id','label'])\n",
    "    for el in predictions:\n",
    "        writer.writerow([el['image_id'], el['label']])\n",
    "    \n",
    "files.download('caliper_cifar10_test_predictions_FangfeiLi.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "caliper_cifar10_architecture_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
